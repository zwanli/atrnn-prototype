{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import argparse\n",
    "import ntpath\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from prettytable import PrettyTable\n",
    "import pandas as pd\n",
    "\n",
    "from preprocessing import is_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: ParserWarning: Both a converter and dtype were specified for column doc_id - only the converter will be used\n/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: ParserWarning: Both a converter and dtype were specified for column citeulike_id - only the converter will be used\n/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: ParserWarning: Both a converter and dtype were specified for column pages - only the converter will be used\n"
     ]
    }
   ],
   "source": [
    "path = '/home/wanli/data/Extended_ctr/citeulike_a_extended/paper_info.csv'\n",
    "null_token = 'NaN'\n",
    "now = datetime.datetime.now()\n",
    "    \n",
    "clean_file_path = path +'.cleaned'\n",
    "\n",
    "\n",
    "# Month converter\n",
    "months = ['apr','aug', 'dec','feb', 'jan' ,'jul' ,'jun' ,'mar' ,'may', 'nov', 'oct', 'sep']\n",
    "month_convert_func = lambda x: x if x in months else null_token\n",
    "\n",
    "def number_convert_func (x):\n",
    "    if is_number(x):\n",
    "        return x\n",
    "    else:\n",
    "        print(x)\n",
    "        return null_token\n",
    "    \n",
    "labels = ['doc_id', 'citeulike_id', 'type', 'pages', 'year']\n",
    "labels_dtype = {'doc_id': np.int32, 'citeulike_id': np.int32, 'type': str, 'pages': np.int32}\n",
    "convert_func= {'pages': number_convert_func, 'doc_id': number_convert_func,\n",
    "               'citeulike_id': number_convert_func}\n",
    "# labels = ['doc_id', 'citeulike_id', 'type', 'journal', 'booktitle', 'series', 'pages', 'year', 'month', 'address']\n",
    "# labels_dtype = {'doc_id': np.int32, 'citeulike_id': np.int32, 'type': str, 'journal': str, 'booktitle': str,\n",
    "#                 'series': str,\n",
    "#                 'pages': np.int32, 'month': str, 'address': str}\n",
    "# convert_func = {'month': month_convert_func, 'pages': number_convert_func, 'doc_id': number_convert_func,\n",
    "#                 'citeulike_id': number_convert_func}\n",
    "\n",
    "df = pd.read_table(clean_file_path, delimiter='\\t', index_col = 'doc_id', usecols=labels,dtype=labels_dtype,\n",
    "                     na_values='\\\\N',na_filter=False,\n",
    "                     converters=convert_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16981, 4)\nciteulike_id    16962\ntype               15\npages             263\nyear               88\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "# count of unique values:\n",
    "print (df.apply(pd.Series.nunique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16981, 4)\nciteulike_id    16962\ntype               15\npages             263\nyear               88\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Filter values with frequency less than min_freq\n",
    "def filter(df, tofilter_list, min_freq):\n",
    "    for col in tofilter_list:\n",
    "        to_keep =  df[col].value_counts().reset_index(name=\"count\").query(\"count > %d\" %min_freq)[\"index\"]\n",
    "        to_keep = to_keep.values.tolist()\n",
    "        df[col] = [x if x in to_keep else 'NaN' for x in df[col] ]\n",
    "    return df\n",
    "\n",
    "tofilter_list = []\n",
    "df = filter(df,tofilter_list,2)\n",
    "print(df.shape)\n",
    "print (df.apply(pd.Series.nunique))\n",
    "\n",
    "# \n",
    "# # Convert catigorical feature into one-hot encoding\n",
    "# def dummmy_df(df, todummy_list):\n",
    "#     for x in todummy_list:\n",
    "#         dummies = pd.get_dummies(df[x], prefix=x, dummy_na=True)\n",
    "#         df = df.drop(x,1)\n",
    "#         df = pd.concat([df,dummies], axis=1)\n",
    "#     return df\n",
    "\n",
    "# todummy_list = ['type']\n",
    "# df_dummy = dummmy_df(df,todummy_list)\n",
    "# # print(df_dummy.head(2))\n",
    "# # print (df2)\n",
    "# print('----------------------------')\n",
    "# # print (df_dummy['journal'].value_counts())\n",
    "# # .reset_index(name=\"count\").query(\"count > 3\")[\"index\"]\n",
    "# print('----------------------------')\n",
    "# print(df_dummy.shape)\n",
    "# print (df_dummy.apply(pd.Series.nunique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NaN' '4' '5' '8' '7' '12' '2' '3' '28' '10' '17' '9' '42' '203' '14' '11'\n '6' '20' '19' '18' '16' '56' '13' '29' '41' '39' '15' '40' '22' '31' '25'\n '23' '59' '27' '48' '33' '35' '34' '51' '30' '60' '43' '32' '24' '83' '1'\n '21' '70' '37' '46' '62' '53' '45' '26' '36' '38' '47' '44' '-1591'\n '-2546' '50' '74' '57' '-275' '49' '573' '-187' '-1825' '-9796' '-1585'\n '-87' '-374' '-395' '-96' '-86' '-284' '-93' '-681' '-387' '-134' '-476'\n '-696' '-1691' '-524' '-94' '-2389' '-177' '-595' '-2189' '-363' '-4495'\n '-292' '-12835' '-688' '-796' '-537' '-15515' '-1595' '-2114' '-191'\n '-144' '-391' '-297' '-10605' '-397' '-1196' '-1994' '-482' '-5935'\n '-2356' '-317' '-616' '-189' '-415' '-1089' '-57' '-693' '90' '54' '52'\n '63' '-385' '-64' '-1297' '55' '97' '65' '-789' '-280' '-173' '77' '-73'\n '-994' '-3289' '-433' '85' '-84' '261' '58' '66' '-388' '-1392' '108' '71'\n '-1070' '-370' '-1267' '-10906' '-88' '-194' '75' '68' '88' '-1896' '-163'\n '-95' '-12465' '-295' '-1940' '93' '73' '-845' '117' '92' '78' '64' '152'\n '-786' '61' '-77' '-1236' '-2393' '-5284' '133' '-10392' '-22' '-269'\n '-1195' '-165' '-208' '-797' '-2184' '-867' '-590' '-747' '-223' '-398'\n '-887' '-1939' '-714' '-695' '-603' '-70' '-151' '5992' '-68' '-262' '-2'\n '3643' '-1391' '132' '-357' '-1891' '-486' '-826' '-406' '-23' '-1473'\n '87' '-33' '-588' '-186' '-289' '-1093' '-286' '-1783' '-1092' '-687'\n '-91' '-1008' '-27' '-490' '-274' '-196' '76' '-188' '104' '-780' '-1062'\n '-896' '-467' '-1082' '-437' '-14' '89' '143' '144' '-795' '-155' '-373'\n '1575' '-471' '-3658' '1836' '-5793' '103' '-738' '72' '-1' '395' '134'\n '-4625' '124' '-378' '553' '-4376' '-140' '-15' '129' '69' '1694' '-1697'\n '-200896192']\n"
     ]
    }
   ],
   "source": [
    "print (df.pages.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16981, 19)\n(16981, 19)\nciteulike_id          16962\npages                   263\nyear                     88\ntype_NaN                  2\ntype_article              2\ntype_book                 2\ntype_booklet              2\ntype_electronic           2\ntype_inbook               2\ntype_incollection         2\ntype_inproceedings        2\ntype_manual               2\ntype_mastersthesis        2\ntype_misc                 2\ntype_phdthesis            2\ntype_proceedings          2\ntype_techreport           2\ntype_unpublished          2\ntype_nan                  1\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_dummy.shape)\n",
    "df_dummy.dropna(inplace=True)\n",
    "print(df_dummy.shape)\n",
    "print (df_dummy.apply(pd.Series.nunique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# min-max normalization:\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# x = df.loc[:,todummy_list].values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# df.loc[:,todummy_list] = pandas.DataFrame(x_scaled)\n",
    "\n",
    "values = df.values\n",
    "imputer = preprocessing.Imputer()\n",
    "transformed_values = imputer.fit_transform(values)\n",
    "# count the number of NaN values in each column\n",
    "print(np.isnan(transformed_values).sum())\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(transformed_values)\n",
    "transformed_df= pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25976, 1893)\n(25976, 1893)\n"
     ]
    }
   ],
   "source": [
    "print(transformed_df.shape)\n",
    "print(x_scaled.shape)\n",
    "# print(transformed_df.nunique())\n",
    "# print (transformed_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25976, 1893)\n(1893,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.491811938722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# training= np.random.randint(2, size=(x_scaled.shape[0],x_scaled.shape[1]+1))\n",
    "# training[:,:-1] = x_scaled\n",
    "\n",
    "y= np.random.randint(2, size=(x_scaled.shape[1]))\n",
    "# y= y.reshape((y.shape[0],1))\n",
    "print(x_scaled.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# evaluate an LDA model on the dataset using k-fold cross validation\n",
    "model = LinearDiscriminantAnalysis()\n",
    "kfold = KFold(n_splits=3, random_state=7)\n",
    "result = cross_val_score(model, np.transpose(np.matrix(x_scaled)), np.transpose(y), cv=kfold, scoring='accuracy')\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}