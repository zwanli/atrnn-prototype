{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from evaluator import Evaluator\n",
    "from utils import textloader\n",
    "from model import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings  /home/wanli/data/glove.6B/glove.6B.200d.txt\nLoading abstracts\n"
     ]
    }
   ],
   "source": [
    "#Read text input\n",
    "batch_size = 1\n",
    "data_loader = textloader('/home/wanli/data/glove.6B/',batch_size)\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "if os.path.exists('abstracrs_word_embeddings_dummy.pkl'):\n",
    "    print('Loading abstracts')\n",
    "    with open('abstracrs_word_embeddings_dummy.pkl', 'rb') as f:\n",
    "        data_loader.all_documents= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ratings\n"
     ]
    }
   ],
   "source": [
    "print('Loading ratings')\n",
    "ratings_path = '/home/wanli/data/Extended_ctr/dummy/users.dat'\n",
    "data_loader.read_dataset(ratings_path,50,1928)#CHANGE ++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n8810\n57\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b6ebfbe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "x = np.random.normal(size = 1000)\n",
    "lengths = []\n",
    "long_docs=0\n",
    "for d in data_loader.all_documents.values():\n",
    "    # if len(d) < 400:\n",
    "    lengths.append(len(d))\n",
    "    if len(d) < 75:\n",
    "        long_docs += 1\n",
    "print(long_docs)\n",
    "print(max(lengths))\n",
    "print(min(lengths))\n",
    "plt.hist(lengths, normed=True, bins=150)\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 125, 175, 225, 275, 325, 375, 425]\n"
     ]
    }
   ],
   "source": [
    "bucket_boundaries = [x for x in range (75,450,50)]\n",
    "print(bucket_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=1\n",
    "training = []\n",
    "label = []\n",
    "for u, v, r, d, step in data_loader.generate_batches(n_steps):\n",
    "    training.append((u,v,d,len(d)))\n",
    "label=np.ones(len(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866\n1866\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assertEqual' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ef51dd3037d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m                                                                      capacity= 2 * batch_size,dynamic_pad= True)\n\u001b[0;32m     24\u001b[0m \u001b[0mexpected_batch_size\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mbatch_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m assertEqual(out_lengths_t.get_shape().as_list(),\n\u001b[0m\u001b[0;32m     26\u001b[0m                  [expected_batch_size])\n\u001b[0;32m     27\u001b[0m assertEqual(data_and_labels_t[0].get_shape().as_list(),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'assertEqual' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# lengths = tf.placeholder(tf.int32, ())\n",
    "batch_size = 128\n",
    "\n",
    "batch_s = batch_size\n",
    "\n",
    "input_text = tf.placeholder(tf.int32, [batch_s,None],name=\"Input_text\")\n",
    "seq_lengths = tf.placeholder(tf.int32, [batch_s])\n",
    "u_idx = tf.placeholder(tf.int32, [batch_s],name=\"U_matrix\")\n",
    "v_idx = tf.placeholder(tf.int32, [batch_s],name=\"V_matrix\")\n",
    "r = tf.placeholder(tf.float32, [batch_s],name=\"R_target\")\n",
    "\n",
    "input_queue = tf.FIFOQueue(\n",
    "      5000, (tf.int32,tf.int32, tf.int32, tf.int32,tf.float32),shapes=([batch_s,30],[batch_s],[batch_s],[batch_s],[batch_s]))\n",
    "input_enqueue_op = input_queue.enqueue((input_text, seq_lengths, u_idx,v_idx,r))\n",
    "input_t, lengths_t, u_idx_t, v_idx_t, r_t = input_queue.dequeue()\n",
    "close_input_op = input_queue.close()\n",
    "\n",
    "\n",
    "\n",
    "seq_len, outputs = tf.contrib.training.bucket_by_sequence_length(\n",
    "    128,tensors=[input_t, lengths_t, u_idx_t, v_idx_t, r_t],allow_smaller_final_batch=True,\\\n",
    "                                                                     batch_size=batch_size,bucket_boundaries=bucket_boundaries, \\\n",
    "                                                                     capacity= 2 * batch_size,dynamic_pad= True)\n",
    "expected_batch_size =  batch_s\n",
    "assertEqual(out_lengths_t.get_shape().as_list(),\n",
    "                 [expected_batch_size])\n",
    "assertEqual(data_and_labels_t[0].get_shape().as_list(),\n",
    "                 [expected_batch_size, data_len])\n",
    "assertEqual(data_and_labels_t[1].get_shape().as_list(),\n",
    "                 [expected_batch_size, labels_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "lengths = []\n",
    "for d in data_loader.all_documents.values():\n",
    "        lengths.append(len(d))\n",
    "print(len(lengths))\n",
    "# with tf.name_scope(\"batch_examples\"):\n",
    "#     seq_len, outputs = tf.contrib.training.bucket_by_sequence_length(lengths,data_loader.all_documents,batch_size,bucket_boundaries, \\\n",
    "#                                                                      capacity= 2 * batch_size,dynamic_pad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, o = sess.run([seq_len,outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All shapes must be fully defined: [TensorShape([Dimension(128), Dimension(None)]), TensorShape([Dimension(128)]), TensorShape([]), TensorShape([]), TensorShape([])]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b523ef4473ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m input_queue = tf.FIFOQueue(\n\u001b[1;32m---> 12\u001b[1;33m           5000, (tf.int32,tf.int32, tf.int32, tf.int32,tf.float32),shapes=[[batch_size,None], (batch_size), (),(),()])\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/data_flow_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, capacity, dtypes, shapes, names, shared_name, name)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \"\"\"\n\u001b[0;32m    669\u001b[0m     \u001b[0mdtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_type_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_shape_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m     \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_name_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     queue_ref = gen_data_flow_ops._fifo_queue_v2(\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/data_flow_ops.py\u001b[0m in \u001b[0;36m_as_shape_list\u001b[1;34m(shapes, dtypes, unknown_dim_allowed, unknown_rank_allowed)\u001b[0m\n\u001b[0;32m     75\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0munknown_dim_allowed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All shapes must be fully defined: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0munknown_rank_allowed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All shapes must be fully defined: [TensorShape([Dimension(128), Dimension(None)]), TensorShape([Dimension(128)]), TensorShape([]), TensorShape([]), TensorShape([])]"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "input_text = tf.placeholder(tf.int32, [batch_size,None],name=\"Input_text\")\n",
    "seq_lengths = tf.placeholder(tf.int32, [batch_size])\n",
    "u_idx = tf.placeholder(tf.int32, [None],name=\"U_matrix\")\n",
    "v_idx = tf.placeholder(tf.int32, [None],name=\"V_matrix\")\n",
    "r = tf.placeholder(tf.float32, [None],name=\"R_target\")\n",
    "\n",
    "in_shape =input_text.get_shape().as_list()\n",
    "\n",
    "input_queue = tf.FIFOQueue(\n",
    "          5000, (tf.int32,tf.int32, tf.int32, tf.int32,tf.float32),shapes=[[batch_size,None], (batch_size), (),(),()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([Dimension(128)]), TensorShape([Dimension(128)]), TensorShape([]), TensorShape([]), TensorShape([])]\n"
     ]
    }
   ],
   "source": [
    "print(input_queue.shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (128, ?) and (128,) are incompatible",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5db792e5a020>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_enqueue_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menqueue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_idx_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_idx_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdequeue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclose_input_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/data_flow_ops.py\u001b[0m in \u001b[0;36menqueue\u001b[1;34m(self, vals, name)\u001b[0m\n\u001b[0;32m    316\u001b[0m       \u001b[1;31m# the `QueueBase` object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    733\u001b[0m     \"\"\"\n\u001b[0;32m    734\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (128, ?) and (128,) are incompatible"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "input_enqueue_op = input_queue.enqueue((input_text, seq_lengths, u_idx,v_idx,r))\n",
    "input_t, lengths_t, u_idx_t, v_idx_t, r_t = input_queue.dequeue()\n",
    "close_input_op = input_queue.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "print(lengths_t.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_output [0]\n[[5 3 4]\n [1 3 4]]\ntarget_output [0]\n[[9 3 4 6]\n [9 3 4 0]]\n\nsource_output [1]\n[[2 3 4]\n [6 3 4]]\ntarget_output [1]\n[[9 3 4 5]\n [9 3 0 0]]\n\nsource_output [2]\n[[3 4]\n [3 4]]\ntarget_output [2]\n[[9]\n [9]]\n\nsource_output [3]\n[[2 3 4]\n [6 3 4]]\ntarget_output [3]\n[[9 3 4 5]\n [9 3 0 0]]\n\nsource_output [4]\n[[5 3 4]\n [1 3 4]]\ntarget_output [4]\n[[9 3 4 6]\n [9 3 4 0]]\n\nsource_output [5]\n[[3 3 3 3 3 3]\n [3 3 3 3 3 3]]\ntarget_output [5]\n[[9 3 3 3 3 3 2]\n [9 3 3 3 3 3 2]]\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SequenceTable:\n",
    "    def __init__(self, data):\n",
    "        # A TensorArray is required as the sequences don't have the same\n",
    "        # length. Alternatively a FIFOQueue can be used.\n",
    "        # Because the data is read more than once by the queue,\n",
    "        # clear_after_read is set to False (but I can't confirm an effect).\n",
    "        # Because the items has diffrent sequence lengths the infer_shape\n",
    "        # is set to False. The shape is then restored in the .read method.\n",
    "        self.table = tf.TensorArray(size=len(data),\n",
    "                                    dtype=data[0].dtype,\n",
    "                                    dynamic_size=False,\n",
    "                                    clear_after_read=False,\n",
    "                                    infer_shape=False)\n",
    "\n",
    "        # initialize table\n",
    "        for i, datum in enumerate(data):\n",
    "            self.table = self.table.write(i, datum)\n",
    "\n",
    "        # setup infered element shape\n",
    "        self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\n",
    "\n",
    "    def read(self, index):\n",
    "        # read index from table and set infered shape\n",
    "        read = self.table.read(index)\n",
    "        read.set_shape(self.element_shape)\n",
    "        return read\n",
    "\n",
    "\n",
    "def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\n",
    "    # bucket_by_sequence_length requires the input_length and tensors\n",
    "    # arguments to be queues. Use a range_input_producer queue to shuffle\n",
    "    # an index for sliceing the input_length and tensors laters.\n",
    "    # This strategy is idendical to the one used in slice_input_producer.\n",
    "    table_index = tf.train.range_input_producer(\n",
    "        int(input_length.get_shape()[0]), shuffle=shuffle\n",
    "    ).dequeue()\n",
    "\n",
    "    # the first argument is the sequence length specifed in the input_length\n",
    "    # I did not find a ue for it.\n",
    "    _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\n",
    "        input_length=tf.gather(input_length, table_index),\n",
    "        tensors=[tensor.read(table_index) for tensor in tensors],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return tuple(batch_tensors)\n",
    "\n",
    "\n",
    "# these values specify the length of the sequence and this controls how\n",
    "# the data is bucketed. The value is not required to be the acutal length,\n",
    "# which is also problematic when using pairs of sequences that have diffrent\n",
    "# length. In that case just specify a value that gives the best performance,\n",
    "# for example \"the max length\".\n",
    "length_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\n",
    "\n",
    "source_table = SequenceTable([\n",
    "    np.asarray([3, 4], dtype=np.int32),\n",
    "    np.asarray([2, 3, 4], dtype=np.int32),\n",
    "    np.asarray([1, 3, 4], dtype=np.int32),\n",
    "    np.asarray([5, 3, 4], dtype=np.int32),\n",
    "    np.asarray([6, 3, 4], dtype=np.int32),\n",
    "    np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\n",
    "])\n",
    "\n",
    "target_table = SequenceTable([\n",
    "    np.asarray([9], dtype=np.int32),\n",
    "    np.asarray([9, 3, 4, 5], dtype=np.int32),\n",
    "    np.asarray([9, 3, 4], dtype=np.int32),\n",
    "    np.asarray([9, 3, 4, 6], dtype=np.int32),\n",
    "    np.asarray([9, 3], dtype=np.int32),\n",
    "    np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\n",
    "])\n",
    "\n",
    "source_batch, target_batch = shuffle_bucket_batch(\n",
    "    length_table, [source_table, target_table],\n",
    "    batch_size=2,\n",
    "    # devices buckets into [len < 3, 3 <= len < 5, 5 <= len]\n",
    "    bucket_boundaries=[3, 5],\n",
    "    # this will bad the source_batch and target_batch independently\n",
    "    dynamic_pad=True,\n",
    "    capacity=2\n",
    ")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "    for i in range(6):\n",
    "        source, target = sess.run((source_batch, target_batch))\n",
    "        print('source_output [{0}]'.format(i))\n",
    "        print(source)\n",
    "        print('target_output [{0}]'.format(i))\n",
    "        print(target)\n",
    "        print('')\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 1208\n20 872\n26 947\n37 46\n13 710\n12 647\n39 832\n20 258\n20 295\n12 1171\n37 467\n39 832\n21 876\n20 872\n13 710\n20 528\n21 751\n20 258\n44 375\n49 537\n10 416\n20 883\n20 885\n21 751\n20 872\n37 46\n46 111\n20 844\n26 947\n21 751\n21 666\n20 258\n21 881\n20 751\n12 1171\n49 657\n20 181\n21 751\n20 295\n4 804\n4 804\n47 277\n21 514\n21 881\n47 277\n21 666\n39 836\n21 737\n49 1310\n20 751\n20 181\n20 181\n20 827\n20 827\n12 41\n20 844\n20 837\n21 876\n20 844\n44 375\n13 63\n46 1597\n21 666\n46 111\n49 1310\n23 110\n37 467\n11 62\n4 804\n10 416\n10 56\n22 822\n23 110\n13 63\n39 832\n47 277\n43 232\n21 881\n23 1208\n49 355\n49 1310\n21 751\n20 872\n43 232\n49 355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def num_samples(path):\n",
    "    c = 0\n",
    "    for record in tf.python_io.tf_record_iterator(path):\n",
    "        example_proto = tf.train.SequenceExample()\n",
    "        example_proto.ParseFromString(record)\n",
    "        c += 1\n",
    "    return c\n",
    "\n",
    "def _parse_function(sequence_example_proto):\n",
    "        context_feature = {'u': tf.FixedLenFeature([], tf.int64),\n",
    "                           'v': tf.FixedLenFeature([], tf.int64),\n",
    "                           'r': tf.FixedLenFeature([], tf.int64),\n",
    "                           'abs_length': tf.FixedLenFeature([], tf.int64)}\n",
    "\n",
    "        sequence_feature = {'abstract': tf.FixedLenSequenceFeature([], tf.int64)}\n",
    "\n",
    "        # Decode the record read by the reader\n",
    "        context_feature, sequence_feature = tf.parse_single_sequence_example(sequence_example_proto,\n",
    "                                                                             context_features=context_feature,\n",
    "                                                                             sequence_features=sequence_feature)\n",
    "        u = tf.cast(context_feature['u'], tf.int32)\n",
    "        v = tf.cast(context_feature['v'], tf.int32)\n",
    "        r = tf.cast(context_feature['r'], tf.int32)\n",
    "        abs_length = tf.cast(context_feature['abs_length'], tf.int32)\n",
    "        abstract = tf.cast(sequence_feature['abstract'], tf.int32)\n",
    "        return u, v, r, abstract, abs_length\n",
    "def input(batch_size):\n",
    "    filename ='/home/wanli/data/Extended_ctr/dummy_test_0.tfrecords'\n",
    "    print ('number of samples {0}'.format(num_samples(filename)))\n",
    "    dataset = tf.contrib.data.TFRecordDataset(filename)\n",
    "    \n",
    "    # Repeat the input indefinitely.\n",
    "    dataset = dataset.repeat()\n",
    "    # Parse the record into tensors.\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    # Shuffle the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    # Generate batches\n",
    "    # dataset = dataset.batch(128)\n",
    "    \n",
    "    # iterator = dataset.make_initializable_iterator()\n",
    "    # print(dataset.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "    # print(dataset.output_shapes)  # ==> \"(10, ((), (100,)))\"\n",
    "    \n",
    "    dataset = dataset.padded_batch(128, padded_shapes=((), (), (), [None], ()))\n",
    "    # Create a one-shot iterator\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return next_element \n",
    "# with tf.Session() as sess:\n",
    "#     for i in range(100):\n",
    "#         record = sess.run(next_element)\n",
    "batch_size = 85\n",
    "next_element = input(batch_size)\n",
    "pairs = {}\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        record = sess.run(next_element)\n",
    "        for j in range(batch_size):\n",
    "            print(record[0][j],record[1][j])\n",
    "            for i in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}